{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NLP-lab :  Plongements de mots (word embeddings)\n",
    "\n",
    "                                            Christopher Kermorvant\n",
    "\n",
    "                            “The meaning of a word can be inferred by the company it keeps”\n",
    "\n",
    "Dans cette série d'exercices, nous allons explorer  trois  plongements (embeddings) de mots :\n",
    "\n",
    "*  [Collobert & Weston](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) https://ronan.collobert.com/senna/\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "* [BERT](https://huggingface.co/bert-base-uncased) \n",
    "\n",
    "   \n",
    "Pour les deux premiers, nous examinerons les mots les plus proches et visualiserons leurs positions dans l'espaces après réduction de dimension. Puis nous procéderons à des [évaluations](https://arxiv.org/pdf/1801.09536.pdf) qualitatives et intrinsèques des embeddings.\n",
    "\n",
    "Enfin nous étudierons les raisonnements par analogies que l'on peut conduire par l'arithmétique sur les embeddings (et leurs biais).\n",
    "\n",
    "Pour BERT, nous étudierons la représentation d'un mot polysémique en fonction de son contexte.\n",
    "\n",
    "Dans le code déjà fourni, ajouter votre code à l'endroit indiqué par `YOUR CODE HERE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "\n",
    "# disable warnings for libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# configure logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Les fichiers d'embeddings pré-entraînés\n",
    "\n",
    "Téléchargez dans `data` les fichiers contenant les embeddings :\n",
    "* Collobert (taille 50) : [collobert_embeddings.txt.zip](https://storage.teklia.com/shared/deepnlp-labs/collobert_embeddings.txt.zip) qui contient les vecteurs d'embeddings  et [collobert_words.lst](https://storage.teklia.com/shared/deepnlp-labs/collobert_words.lst) qui contient les mots associés;\n",
    "* Glove (taille 50):  [glove.6B.50d.txt.zip](https://storage.teklia.com/shared/deepnlp-labs/glove.6B.50d.txt.zip) qui contient à la fois les vecteurs et les mots.\n",
    "\n",
    "Il faut décompresser les fichiers pour pouvoir les charger.\n",
    "\n",
    "N'hésitez pas à ouvrir les fichiers pour voir ce qu'ils contiennent (c'est parfois surprennant).\n",
    "\n",
    "#### Question : \n",
    ">* Donner la taille des fichiers d'embeddings avant unzip\n",
    ">* En explorant le contenu des fichiers d'embedding, donner le nombre de mots pour lesquels ces fichiers fournissent des embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avant le unzip la taille des fichiers est de 67 618 Ko pour glove\n",
    "et de 24 275 Ko pour collobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nombre_mots_embedding(fichier):\n",
    "    with open(fichier,\"r\",encoding=\"utf-8\") as f:\n",
    "        nombre_mots = sum(1 for ligne in f)\n",
    "    return nombre_mots\n",
    "    \n",
    "nombre_mots_embedding(\"collobert_embeddings.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration des embeddings\n",
    "\n",
    "### Liste des mots les plus proches\n",
    "\n",
    "L'objectif de cet exercice est de lister les mots les plus proches d'un mot donné pour l'embeddings Collobert. Dans un premier temps, nous allons charger les vecteurs de l'embedding Collobert dans un array numpy et les mots associés dans une liste python. Ensuite, nous utiliserons la structure de données [KDTree de scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) pour faire une recherche rapide des vecteurs les plus proches d'une série de mots.\n",
    "\n",
    "### Chargement des embeddings\n",
    "\n",
    "#### Question : \n",
    ">* charger les vecteurs d'embeddings à partir du fichier `data/collobert_embeddings.txt` en utilisant la fonction numpy [genfromtxt](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html)\n",
    ">* charger dans une liste python les mots associés aux vecteurs à partir du fichier `data/collobert_words.lst` (avec `open()` et `readlines()`)\n",
    ">* vérifiez que les tailles sont correctes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130000\n",
      "130000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# YOUR CODE HERE\n",
    "embedding = np.genfromtxt('collobert_embeddings.txt')\n",
    "\n",
    "with open(\"collobert_words.lst\", 'r', encoding='utf-8') as collobert_words:\n",
    "    collobert_words = [line.strip() for line in collobert_words.readlines()]\n",
    "\n",
    "print(len(collobert_words))\n",
    "print(embedding.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les arbres KD (KD tree) sont une structure de données très efficace pour stocker de grands ensemble de points dans une espace multi-dimensionnel et faire des recherches très efficaces de plus proches voisins. \n",
    "\n",
    "#### Question \n",
    "> * Initialisez la structure de [KDTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) avec les vecteurs d'embeddings de Collobert\n",
    "> * En utilisant la fonction [tree.query](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query.html#scipy.spatial.KDTree.query), afficher les 5 mots les plus proches des mots suivants : 'mother', 'computer', 'dentist', 'war', 'president', 'secretary', 'nurse' \n",
    "     * *Indice : vous pouvez utiliser la fonction `collobert_words.index(w)` pour obtenir l'indice d'un mot dans la liste des mots*\n",
    "> * Créer une liste `words_plus_neighbors` contenant les mots et tous leurs voisins (pour la question suivante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "# YOUR CODE HERE\n",
    "tree = spatial.KDTree(embedding)\n",
    "\n",
    "def mot_proche(mot, k=5):\n",
    "    if mot not in collobert_words:\n",
    "        print(\"mot pas dedans\")\n",
    "    \n",
    "    index = collobert_words.index(mot)\n",
    "    vecteur = embedding[index]\n",
    "    distances, indices = tree.query(vecteur, k=k+1)\n",
    "    words_plus_neighbors = []\n",
    "    words_plus_neighbors.append(mot)\n",
    "    for i in range(1,k+1):\n",
    "        print(f\"{collobert_words[indices[i]]}\")\n",
    "        words_plus_neighbors.append(collobert_words[indices[i]])\n",
    "\n",
    "    return words_plus_neighbors, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daughter\n",
      "wife\n",
      "father\n",
      "husband\n",
      "son\n",
      "\n",
      "\n",
      "calculate\n",
      "decode\n",
      "computes\n",
      "encode\n",
      "optimize\n",
      "\n",
      "\n",
      "pharmacist\n",
      "midwife\n",
      "physician\n",
      "housekeeper\n",
      "veterinarian\n",
      "\n",
      "\n",
      "revolution\n",
      "death\n",
      "court\n",
      "independence\n",
      "history\n",
      "\n",
      "\n",
      "governor\n",
      "chairman\n",
      "mayor\n",
      "secretary\n",
      "senator\n",
      "\n",
      "\n",
      "minister\n",
      "treasurer\n",
      "chairman\n",
      "commissioner\n",
      "undersecretary\n",
      "\n",
      "\n",
      "physician\n",
      "veterinarian\n",
      "dentist\n",
      "surgeon\n",
      "midwife\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['nurse', 'physician', 'veterinarian', 'dentist', 'surgeon', 'midwife'],\n",
       " array([ 81489,  88257, 123430,  30873, 112584,  74121]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_proche(\"mother\")\n",
    "print(\"\\n\")\n",
    "mot_proche(\"compute\")\n",
    "print(\"\\n\")\n",
    "mot_proche(\"dentist\")\n",
    "print(\"\\n\")\n",
    "mot_proche(\"war\")\n",
    "print(\"\\n\")\n",
    "mot_proche(\"president\")\n",
    "print(\"\\n\")\n",
    "mot_proche(\"secretary\")\n",
    "print(\"\\n\")\n",
    "mot_proche(\"nurse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation avec T-SNE\n",
    "\n",
    "Les embeddings sont des vecteurs de plusieurs centaines de dimensions. Il n'est donc pas possible de les visualiser dans leur espace d'origine. Il est par contre possible d'appliquer des algorithmes de réduction de dimension pour les visualiser en 2 ou 3 dimension. Un des algorithmes de réduction de dimension permettant une visualisation en 2D est [tSNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). \n",
    "\n",
    "#### Question\n",
    "> * créer un object `word_vectors` de type `np.array` à partir d'une liste contenant tous les embeddings des mots de la liste `words_plus_neighbors`\n",
    "> * créer un objet tSNE à partir de la librairie `from sklearn.manifold import TSNE` avec les paramètres `random_state=0`, `n_iter=2000` et `perplexity=15.0` pour une visualisation en 2 dimensions\n",
    "> * Calculer *T* la transformation tSNE des vecteur `word_vectors` en appliquant la function `.fit_transform(word_vectors)` à l'objet tSNE. Cette fonction estime les paramètres de la transformation tSNE et retourne la représentation en dimension réduite des vecteurs utilisés pour l'estimation.\n",
    "> * Utiliser la fonction `scatterplot` de [seaborn](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) pour représenter les points en 2 dimensions  et ajouter les labels des mots avec la function `plt.annotate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daughter\n",
      "wife\n",
      "father\n",
      "husband\n",
      "son\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "# display matplotlib graphics in notebook\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# retrieve the word representation\n",
    "# YOUR CODE HERE\n",
    "words_plus_neighbors, indices = mot_proche(\"mother\")\n",
    "word_vectors = embedding[indices]\n",
    "# create the tSNE transform\n",
    "# YOUR CODE HERE\n",
    "# fit and transform the word vectors, store in T\n",
    "# YOUR CODE HERE\n",
    "T= TSNE(random_state=0, n_iter=2000, perplexity = 2.0).fit_transform(word_vectors)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "sns.set(rc={'figure.figsize':(14, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "sns.scatterplot(x=T[:, 0], y=T[:, 1])\n",
    "\n",
    "for label, x, y in zip(words_plus_neighbors, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation des embeddings \n",
    "\n",
    "### Évaluation intrinsèque\n",
    "\n",
    "[A Survey of Word Embeddings Evaluation Methods](https://arxiv.org/pdf/1801.09536.pdf), Bakarov, 2018.\n",
    "\n",
    "\n",
    ">les distances entre les mots dans un espace vectoriel pourraient être évaluées à l'aide des jugements heuristiques humains sur les distances sémantiques réelles entre ces mots (par exemple, la distance entre tasse et gobelet définies dans un intervalle continu 0, 1 serait 0.8 puisque ces mots sont synonymes, mais pas vraiment la même chose).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement des datasets pré-établis et annotés manuellement\n",
    "\n",
    "Nous allons utiliser 4 jeux de données  pour évaluer la qualité des embeddings : [MEN](http://clic.cimec.unitn.it/~elia.bruni/MEN.html), [WS353R](http://www.aclweb.org/anthology/N09-1003.pdf), [SimLex999](http://leviants.com/ira.leviant/MultilingualVSMdata.html) et [MTurk](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.205.8607&rep=rep1&type=pdf). \n",
    "\n",
    "\n",
    "Ces jeux de données contiennent des paires de mots dont la proximité sémantique a été évaluée manuellement par des humains. Pour chaque dataset, dataset.X contient une liste de paires de mots et dataset.y contient le score de proximité pour chaque paire.\n",
    "\n",
    "* MEN, 3 000 paires évaluées par relation sémantique avec une échelle discrète de 0 à 50\n",
    "* SimLex-999, 999 paires évaluées avec un fort respect pour la similarité sémantique avec une échelle de 0 à 10\n",
    "* MTurk-287, 287 paires évaluées par relation sémantique avec une échelle de 0 à 5\n",
    "* WordSim-353, 353 paires évaluées par similarité sémantique (cependant, certains chercheurs trouvent les instructions pour les évaluateurs ambiguës en ce qui concerne la similarité et l'association) sur une échelle de 0 à 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MEN dataset: similarity and relatedness\n",
      "Downloading WS353 dataset: attributional and relatedness similarity\n",
      "Downloading SimLex999 dataset: attributional similarity\n",
      "Downloading MTurk dataset: attributional similarity\n",
      "\n",
      " MEN : 3000 items\n",
      "     sun, sunlight : [10.]\n",
      "     automobile, car : [10.]\n",
      "     river, water : [9.8]\n",
      "     stair, staircase : [9.8]\n",
      "\n",
      " WS353R : 252 items\n",
      "     computer, keyboard : 7.62\n",
      "     Jerusalem, Israel : 8.46\n",
      "     planet, galaxy : 8.11\n",
      "     canyon, landscape : 7.53\n",
      "\n",
      " SimLex999 : 999 items\n",
      "     old, new : 1.58\n",
      "     smart, intelligent : 9.2\n",
      "     hard, difficult : 8.77\n",
      "     happy, cheerful : 9.55\n",
      "\n",
      " MTurk : 287 items\n",
      "     episcopal, russia : 5.5\n",
      "     water, shortage : 5.428571428\n",
      "     horse, wedding : 4.533333334\n",
      "     plays, losses : 6.4\n"
     ]
    }
   ],
   "source": [
    "# custom functions\n",
    "import similarity\n",
    "\n",
    "similarity_tasks = {\n",
    "    \"MEN\": similarity.fetch_MEN(),\n",
    "    \"WS353R\": similarity.fetch_WS353(which=\"relatedness\"),\n",
    "    \"SimLex999\": similarity.fetch_SimLex999(),\n",
    "    \"MTurk\": similarity.fetch_MTurk(),\n",
    "}\n",
    "\n",
    "for name, dataset in similarity_tasks.items():\n",
    "    print('\\n', name, ':',len(dataset.X),'items')\n",
    "    for data, score in zip(dataset.X[:4], dataset.y[:4]):\n",
    "        print(' '*4, ', '.join(data), ':', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats évaluation intrinsèque\n",
    "\n",
    "Notre objectif est de comparer les similarités entre les paires de mots des datasets calculées à partir des embeddings et celles données par les annotateurs humains. Si un embedding prédit les similarités de la même manière que les humains, on estime qu'il est bon. On peut donc calculer la corrélation entre la proximité donné par l'embedding et celle donnée par les humains pour chaque paire de mots du dataset.\n",
    "\n",
    "Pour cet excercice, nous allons utiliser  le classe [Embeddings](https://polyglot.readthedocs.io/en/latest/polyglot.mapping.html#module-polyglot.mapping.embeddings) de polyglot. Pour charger un embeddind avec cette classe : \n",
    "\n",
    "`glove_embeddings =  Embedding.from_glove('data/glove.6B.50d.txt')`\n",
    "\n",
    "Pour pouvoir charger les embeddings de Collobert de la même manière, il faut mettre les mots et les vecteurs dans un seul fichier, par exemple avec la commande linux `paste`:\n",
    "\n",
    "`paste -d ' ' collobert_words.lst collobert_embeddings.txt > collobert.txt`\n",
    "\n",
    "\n",
    "\n",
    "#### Question\n",
    "\n",
    "> * pour chaque embedding Collober et Glove, et chaque dataset (MEN, WS353R, SimLex999 et MTurk), calculer la similarité entre les proximités données par l'embedding et celles données par les humains. On utilisera la fonction `similarity.evaluate_similarity(word_embeddings, dataset.X, dataset.y)` qui renvoit le [coefficient de correlation de Spearman](https://fr.wikipedia.org/wiki/Corr%C3%A9lation_de_Spearman).\n",
    "> * stocker les scores  pour chaque embedding et chaque dataset dans une liste `similarity_results = []` sous forme d'un dictonnaire : `similarity_results.append({'Embeddings': embeddings_name, 'Dataset': name, 'Score': score})`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embeddings_name, embeddings \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollobert\u001b[39m\u001b[38;5;124m'\u001b[39m, collobert_embeddings), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove\u001b[39m\u001b[38;5;124m'\u001b[39m, glove_embeddings)]:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# loop on tasks\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, dataset \u001b[38;5;129;01min\u001b[39;00m similarity_tasks\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# compute similarity\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         similarity_results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmbeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: embeddings_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m: name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m: score})\n",
      "File \u001b[0;32m~/work/nlp-lab-text-embedding/similarity.py:391\u001b[0m, in \u001b[0;36mevaluate_similarity\u001b[0;34m(w, X, y)\u001b[0m\n\u001b[1;32m    388\u001b[0m     w \u001b[38;5;241m=\u001b[39m Embedding\u001b[38;5;241m.\u001b[39mfrom_dict(w)\n\u001b[1;32m    390\u001b[0m missing_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 391\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[38;5;241m.\u001b[39mword_id\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query_word \u001b[38;5;129;01min\u001b[39;00m query:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'vocabulary'"
     ]
    }
   ],
   "source": [
    "# embedding functions\n",
    "from polyglot.mapping import Embedding\n",
    "\n",
    "similarity_results = []\n",
    "# Load both embeddings with Embedding.from_glove from Polyglot\n",
    "# YOUR CODE HERE\n",
    "glove_embeddings =  Embedding.from_glove('glove.6B.50d.txt')\n",
    "with open(\"collobert_embeddings.txt\", 'r', encoding='utf-8') as f:\n",
    "    collobert_embeddings = [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "# Loop on embeddings\n",
    "for embeddings_name, embeddings in [('collobert', collobert_embeddings), ('glove', glove_embeddings)]:\n",
    "    # loop on tasks\n",
    "    for name, dataset in similarity_tasks.items():\n",
    "        # compute similarity\n",
    "        # YOUR CODE HERE\n",
    "        score = similarity.evaluate_similarity(word_embeddings, dataset.X, dataset.y)\n",
    "        similarity_results.append({'Embeddings': embeddings_name, 'Dataset': name, 'Score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des résultats de similarité\n",
    "\n",
    "Le code suivant permet de visualiser les coefficients de corrélation pour chaque dataset sur les différents jeux de test.\n",
    "\n",
    "#### Question\n",
    "> * Quel est selon ces métriques le meilleur embedding ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(similarity_results, orient='columns')\n",
    "df\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "colors = [\"#e74c3c\", \"#75d9fc\", \"#b4e0ef\", \"#34495e\", \"#e74c3c\", \"#2ecc71\"]\n",
    "ax = sns.barplot(x=\"Dataset\", y=\"Score\", hue=\"Embeddings\", data=df, errwidth=0, palette=sns.color_palette(colors))\n",
    "\n",
    "\n",
    "ax.legend(loc=9, bbox_to_anchor=(0.5, -0.1), ncol=3, fancybox=True, shadow=False)\n",
    "ax.set(xlabel=\"\", ylabel=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation d'analogies\n",
    "\n",
    "Notre objectif est maintenant d'explorer les relations sémantiques induites par l'arithmétique sur les embeddings. Nous allons donc explorer les analogies induites par les embeddings sous forme de raisonnement du type : \"l'homme est au roi ce que la femme est à ?\", la réponse étant \"la reine\". On peut calculer la réponse avec les représentations fournies par l'embedding par :  \n",
    "\n",
    "`v = vecteur(roi)-vecteur(homme)+vecteur(femme)`. \n",
    "\n",
    "La réponse étant alors le mot dont la représentation est la plus proche du vecteur `v`. Pour trouver le mot dont le vecteur est le plus proche de `v`, il faut définir une distance dans l'espace des embeddings. Nous utiliserons la [similarité cosinus](https://fr.wikipedia.org/wiki/Similarit%C3%A9_cosinus)\n",
    "\n",
    "#### Question\n",
    ">* Implémenter la similarity cosinus à l'aide des fonctions [np.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html#numpy.dot) et [np.linalg.norm](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html#numpy.linalg.norm)\n",
    ">* Appliquer le calcul d'analogies sur les triplets proposés ou ceux de votre choix. Observez-vous [ce phénomène](https://arxiv.org/pdf/1607.06520.pdf) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cosine_similarity(a,b):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def sorted_by_similarity(word_embeddings, base_vector):\n",
    "    \"\"\"Returns words sorted by cosine distance to a given vector, most similar first\"\"\"\n",
    "    words_with_distance = [(my_cosine_similarity(base_vector, word_embeddings[w]), w) \n",
    "                           for w in word_embeddings.vocabulary]\n",
    "\n",
    "    return sorted(words_with_distance, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "def is_redundant(word):\n",
    "    return (\n",
    "        word_1.lower() in word.lower() or\n",
    "        word_2.lower() in word.lower() or\n",
    "        word_3.lower() in word.lower())\n",
    "\n",
    "\n",
    "pairs = [(['man', 'woman'], 'king'), \n",
    "         (['man', 'programmer'], 'woman'), \n",
    "         (['father', 'doctor'], 'mother'),\n",
    "         (['father', 'facebook'], 'mother')\n",
    "        ]\n",
    "\n",
    "words_and_responses = []\n",
    "\n",
    "# Note : you may need to update the following line with your Polyglot Embeddings\n",
    "for embeddings_name, embeddings in [('collobert', collobert_embeddings), ('glove', glove_embeddings)]:\n",
    "    for pair in pairs:\n",
    "        word_1, word_2, word_3 = pair[0][0], pair[0][1], pair[1]\n",
    "        \n",
    "        closest = sorted_by_similarity(embeddings, \n",
    "                                       embeddings[word_2] - embeddings[word_1] + \n",
    "                                       embeddings[word_3])[:10]\n",
    "\n",
    "        closest = [(dist, w) for (dist, w) in closest if not is_redundant(w)] #\n",
    "        \n",
    "        print(\"{} + {} - {} = ? {}\".format(word_2, word_3, word_1, closest[0][1]))\n",
    "        words_and_responses += [word_1, word_2, word_3,closest[0][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des analogies\n",
    "\n",
    "Les relations d'analogies peuvent se visualiser dans l'espace des embeddings après réduction de dimension, par exemple avec tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : you may need to update the following line with your Polyglot Embeddings\n",
    "for embeddings_name, embeddings in [('collobert', collobert_embeddings), ('glove', glove_embeddings)]:\n",
    "    \n",
    "    word_vectors = np.array([embeddings[word] for word in words_and_responses[:4]])\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=0, n_iter=1000, perplexity=3.0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    T = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(6, 6)})\n",
    "    sns.set(font_scale=1.3)\n",
    "\n",
    "    sns.scatterplot(x=T[:, 0], y=T[:, 1])\n",
    "    \n",
    "    for label, x, y in zip(words_and_responses, T[:, 0], T[:, 1]):\n",
    "        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des embeddings de BERT\n",
    "\n",
    "BERT a été un des premiers modèles de langue Transformer, entraînés sur de gros corpus, disponible librement. De nombreux modèles sont disponibles sur HuggingFace.\n",
    "\n",
    "Comme BERT est un modèle contextuel, il est nécessaire de lui faire prédire des phrases entières pour étudier les embeddings de mots qu'il produit. Dans cette section, nous allons comparer les embeddings obtenus pour des mots polysémiques en fonction de la phrase dans laquelle ils sont utilisés.\n",
    "\n",
    "En anglais, *plant* possède deux sens : celui d'usine et celui d'un végétal. Avec un embedding non contextuel, de type Glove ou Colobert, ces deux sens du mot plus sont associés à un identique embedding. Avec BERT, nous allons voir que le même mot peut avoir plusieurs embeddings en fonction du contexte.\n",
    "\n",
    "First, load the BERT model and tokenizer from HuggingFace : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model \n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # to access the hidden states\n",
    "                                  )\n",
    "# set the model to \"evaluation\" mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Les modèles de langues sont entrainés avec un découpe spécifique des phrases en token. Ces tokens peuvent être des mots ou des parties de mots. Il est nécessaire d'utiliser le tokenizer correspondant à chaque model.\n",
    "\n",
    "tokenizer.vocab.keys() donne la liste de tous les tokens connus du modèle de langue. \n",
    "\n",
    "#### Question\n",
    ">* combien de token différents sont connu du tokenizer de BERT ?\n",
    ">* affichez une centaine de token aléatoirement. Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# number of token in tokenizer\n",
    "# YOU CODE HERE\n",
    "# sample of 100 tokens\n",
    "# YOU CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer découpe les phrases et transforme les éléments (mots ou sous-mots) en indice. \n",
    "\n",
    "BERT peut traiter plusieurs phrases mais il faut lui indiquer le découpage en phrases (segment) avec un indice : 0 pour la première phrases, 1 pour la deuxième. \n",
    "\n",
    "Deux tokens spécifiques doivent être aussi ajoutés : \n",
    "* [CLS], un token spécifique utilisé pour la classification de phrase\n",
    "* [SEP], le token de fin de phrase.\n",
    "\n",
    "#### Question\n",
    ">* Appliquer la fonction bert_tokenize sur les 3 phases et conservez les 3 vecteurs (index, token, segment)\n",
    ">* Affichez ces informations pour chacune des phrases et vérifier que le mot *plant* a bien le même indice de token dans les deux phrases où il apparait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt1 = \"The plant has reached its maximal level of production.\"\n",
    "snt2 = \"The cars are assembled inside the factory.\"\n",
    "snt3 = \"A plant needs sunlight and water to grow well.\"\n",
    "\n",
    "\n",
    "def bert_tokenize(snt):\n",
    "    \"\"\" Apply the BERT tokenizer to a list of words representing a sentence\n",
    "        and return 3 lists: \n",
    "        - list of token indx\n",
    "        - list of token for debugging, not used by the BERT model\n",
    "        - list of sentence index\n",
    "        \"\"\"\n",
    "    # Add the special tokens.\n",
    "    tagged_snt = \"[CLS] \" + snt + \" [SEP]\" \n",
    "    # Tokenize\n",
    "    tokenized_snt = tokenizer.tokenize(tagged_snt)\n",
    "    # convert tokens to indices\n",
    "    indexed_snt = tokenizer.convert_tokens_to_ids(tokenized_snt)\n",
    "    # mark the words in sentence.\n",
    "    segments_ids = [1] * len(tokenized_snt)\n",
    "\n",
    "    return (indexed_snt, tokenized_snt, segments_ids)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inférence\n",
    "\n",
    "Pour calculer les embeddings, il est nécessaire de faire une prédiction avec le modèle BERT sur une phrase complète. La fonction *predict_hidden* convertit les listes d'indices de token et de segment en tenseur pytorch et applique le modèle. \n",
    "\n",
    "Le modème utilisé est un modèle à 12 couches. Nous allons utiliser la dernière couche caché du modèle comme embedding pour représenter les mots. D'autres solutions serait possible, comme une concaténation ou une moyene de plusieurs couches.\n",
    "\n",
    "\n",
    "#### Question\n",
    ">* Appliquer le modèle à chacune des 3 phrases et stocker les embeddings obtenus (tenseurs)\n",
    ">* Afficher la dimension des tenseurs obtenus. Quelle est la dimension du vecteur d'embedding pour chaque mot ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_hidden(indexed_snt, segments_ids):\n",
    "    \"\"\"Apply the BERT model to the input token indices and segment indices\n",
    "        and return the last hidden layer\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_snt])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        one_hidden_layer = hidden_states[12][0]\n",
    "        \n",
    "    return one_hidden_layer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La couche cachée renvoyée par la fonction *predict_hidden* est un tenseur contenant pour chaque token de la phrase d'entrée un vecteur contextuel le représentant. On peut utiliser ce vecteur pour représenter le sens de ce mot en fonction de son contexte. Nous allons comparer la représentation du mot polysémique *plant* en fonction de son contexte.\n",
    "\n",
    "#### Question\n",
    ">* En utilisant la [distance cosinus](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html), calculer les distances suivantes:\n",
    ">   * distance entre *plant* dans la phrase 1 (plant-factory) et *plant* dans la phrase 3 (plant-vegetal)\n",
    ">   * distance entre *plant* dans la phrase 1 (plant-factory) et *factory* dans la phrase 2 \n",
    ">   * distance entre *plant* dans la phrase 1 (plant-factory) et *production* dans la phrase 2 \n",
    ">   * distance entre *plant* dans la phrase 3 (plant-vegetal) et *production* dans la phrase 2 \n",
    "> * Comment interprêter ces distances ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
